import logging

from overrides import overrides
from argparse import _SubParsersAction
from typing import Dict

from spr_pick.cli.cmds import Command
from spr_pick.eval import DenoiserEvaluator
from spr_pick.cfg import DEFAULT_RUN_DIR
from spr_pick.params import ConfigValue

logger = logging.getLogger(__name__)


class EvaluateCommand(Command):
    @overrides
    def configure(self, parser: _SubParsersAction):
        cmd_parser = parser.add_parser(self.cmd(), help="Evaluate a pre-trained model.")
        cmd_parser.add_argument(
            "--model",
            "-m",
            required=True,
            help="Path to model weights or training file.",
        )
        cmd_parser.add_argument(
            "--dataset",
            "-d",
            required=True,
            help="Path to either a hdf5 file generated by 'dataset_tool_h5.py' or a folder of images.",
        )
        cmd_parser.add_argument(
            "--runs_dir",
            default=DEFAULT_RUN_DIR,
            help="Directory in which the output directory is generated."
        )
        cmd_parser.add_argument(
            "--batch_size",
            type=int,
            help="Batch size to use, will default to that used while training.",
        )
        cmd_parser.add_argument(
            "--gt_dataset", "-g", help = "ground truth image",)

        cmd_parser.add_argument(
            "--nms",
            "-nms",
            type=int,
            help="non maximum suppression radius",)

        cmd_parser.add_argument(
            "--num",
            "-num",
            type=int,
            default=10,
            help="Number of eval samples during training")
        


    @overrides
    def execute(self, args: Dict):
        evaluator = DenoiserEvaluator(args["model"],runs_dir=args["runs_dir"])
        if args.get("batch_size", None) is not None:
            evaluator.cfg[ConfigValue.TEST_MINIBATCH_SIZE] = args["batch_size"]
        if args.get("nms", None) is not None:
            evaluator.cfg[ConfigValue.NMS] = args["nms"]
        if args.get("num", None) is not None:
            evaluator.cfg[ConfigValue.NUM_EVAL] = args["num"]

        evaluator.set_test_data(args["dataset"])
        evaluator.set_test_gt_data(args["gt_dataset"])
        evaluator.evaluate()

    @overrides
    def cmd(self) -> str:
        return "eval"
