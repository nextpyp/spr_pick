import logging
import spr_pick

from overrides import overrides
from argparse import _SubParsersAction
from typing import Dict

from spr_pick.cli.cmds import Command
from spr_pick.params import NoiseAlgorithm, NoiseValue, ConfigValue
from spr_pick.train import resume_run, DenoiserTrainer
from spr_pick.cfg import DEFAULT_RUN_DIR

logger = logging.getLogger(__name__)


class TrainCommand(Command):

    START_CMD = "start"
    RESUME_CMD = "resume"
    # DETECT_CMD = "detect"
    # JOINT_CMD = "joint"
    # SSDET_CMD = "ssdet"

    @overrides
    def configure(self, parser: _SubParsersAction):
        cmd_parser = parser.add_parser(
            self.cmd(), help="Train or resume training of a Denosier model. "
        )
        # Split behaviour into start and resume
        action_parsers = cmd_parser.add_subparsers(dest="train_cmd", required=True)
        start_parser = action_parsers.add_parser(
            TrainCommand.START_CMD, help="start command help"
        )
        self.add_shared_args(start_parser, True)
        start_parser.add_argument(
            "--algorithm",
            "-a",
            required=True,
            help="The algorithm to train.",
            choices=[c.value for c in spr_pick.utils.list_constants(NoiseAlgorithm)],
        )
        start_parser.add_argument(
            "--noise_style",
            "-n",
            required=True,
            help="Noise style using a string configuration in the format: {noise_type}{args} "
            + "where {args} are the arguments passed to the noise function. The formats for the "
            + "supported noise types include 'gauss{SD}', 'gauss{MIN_SD}_{MAX_SD}', 'poisson{LAMBDA}' "
            + "'poisson{MIN_LAMBDA}_{MAX_LAMBDA}'. If parameters contain a decimal point they are "
            + "treated as floats. This means the underlying noise adding method will not attempt to "
            + "scale them (/ 255). By default noise algorithms will use truncated versions (clipped "
            + "prior to training) -  Append '_nc' to avoid this.",
        )
        start_parser.add_argument(
            "--noise_value",
            help="[joint] Whether the noise value should be estimated.",
            choices=[c.value for c in spr_pick.utils.list_constants(NoiseValue)],
        )
        start_parser.add_argument('--dn_only', action='store_true',
                             help='denoising only')

        start_parser.add_argument(
            "--runs_dir",
            default=DEFAULT_RUN_DIR,
            help="Directory in which the output directory is generated."
        )

        resume_parser = action_parsers.add_parser(
            TrainCommand.RESUME_CMD,
            help="Resume the training of a model. Note that configuration arguments "
            + "used on start are valid when resuming but may cause undefined behaviour - "
            + "use these for redefining data locations if needed.",
        )
        
        resume_parser.add_argument(
            "run_dir",
            help="Path to run directory to resume, the latest *.training file will be used.",
        )

        
        # joint_parser.add_argument(
        #     "run_dir", help="Path to run directory to resume, the latest *.training file will be used.",)
       


        self.add_shared_args(resume_parser, False)

    def add_shared_args(self, parser: _SubParsersAction, start: bool):
        parser.add_argument(
            "--train_dataset",
            "-t",
            required=start,
            help="Path to training dataset. This can be either a hdf5 file generated by "
            + "'dataset_tool_h5.py' or a folder of images. Note that images smaller than "
            + "the patch size will be padded using reflection when a folder is used.",
        )


        parser.add_argument(
            "--alpha",
            "-ap",
            type=float,
            required=start,
            help="alpha value")

        parser.add_argument(
            "--tau",
            "-tau",
            type=float,
            required=start,
            help="tau value for positive unlabeled learning - percentage of positives")

        parser.add_argument(
            "--train_gt",
            "-gt",
            help="Path to ground truth dataset",)


        parser.add_argument(
            "--train_label",
            "-l",
            required=start,
            help="Path to training dataset labels. This can be either a hdf5 file generated by "
            + "'dataset_tool_h5.py' or a folder of images. Note that images smaller than "
            + "the patch size will be padded using reflection when a folder is used.",
        )

        
        parser.add_argument(
            "--validation_dataset",
            "-v",
            help="Path to validation dataset. This can be either a hdf5 file generated by "
            + "'dataset_tool_h5.py' or a folder of images.",
        )

        parser.add_argument(
            "--validation_label",
            "-vl",
            help="Path to validation label. This can be either a hdf5 file generated by "
            + "'dataset_tool_h5.py' or a folder of images.",
        )

       

        parser.add_argument(
            "--validation_gt",
            "-vgt",
            help="Path to validation ground truth dataset")
        
        parser.add_argument(
            "--iterations",
            "-iter",
            required=start,
            type=int,
            help="Number of joint training iterations")

        parser.add_argument(
            "--num",
            "-num",
            type=int,
            default=1,
            help="Number of eval samples during training")

        parser.add_argument(
            "--lr",
            "-lr",
            type=float,
            help="learning rate")
        
        parser.add_argument(
            "--nms",
            "-nms",
            type=int,
            help="non_maximum suppression radius")

        parser.add_argument(
            "--bb",
            "-bb",
            type=int,
            help="bounding box radius for particle of interests")
        
        parser.add_argument(
            "--eval_interval",
            type=int,
            help="Number of iterations between evaluations. Should be divisible by "
            + "training batch size.",
        )
        parser.add_argument(
            "--checkpoint_interval",
            type=int,
            help="Number of iterations between saving checkpoints. Should be divisible by "
            + "training batch size.",
        )
        parser.add_argument(
            "--print_interval",
            type=int,
            help="Number of iterations between printing ongoing results to command line and "
            + "Tensorboard, should be divisible by training batch size.",
        )
        parser.add_argument(
            "--train_batch_size",
            type=int,
            help="Batch size to use for training images.",
        )
        parser.add_argument(
            "--validation_batch_size",
            type=int,
            help="Batch size to use for validation images.",
        )
        parser.add_argument(
            "--patch_size", type=int, help="Patch size to use for training (square).",
        )
        parser.add_argument(
            "--fraction", help = "percent fraction of frames.")


    @overrides
    def execute(self, args: Dict):
        if args["train_cmd"] == "start":
            if args["algorithm"] == "ssdn" and args.get("noise_value", None) == None:
                args["PARSER"].error("SSDN requires --noise_value")
            cfg = spr_pick.cfg.base()
            if args.get("algorithm", None) is not None:
                cfg[ConfigValue.ALGORITHM] = NoiseAlgorithm(args["algorithm"])
            if args.get("noise_style", None) is not None:
                cfg[ConfigValue.NOISE_STYLE] = args["noise_style"]
            if args.get("noise_value", None) is not None:
                cfg[ConfigValue.NOISE_VALUE] = NoiseValue(args["noise_value"])
            if args.get("mono", None) is not None:
                cfg[ConfigValue.IMAGE_CHANNELS] = 1
            if args.get("diagonal", None) is not None:
                cfg[ConfigValue.DIAGONAL_COVARIANCE] = args["diagonal"] 
            if args.get("lr", None) is not None:
                cfg[ConfigValue.LEARNING_RATE] = args["lr"]
            if args.get("bb", None) is not None:
                cfg[ConfigValue.BB] = args["bb"]
            if args.get("nms", None) is not None:
                cfg[ConfigValue.NMS] = args["nms"]
            if args["dn_only"]:
                trainer = DenoiserTrainer(cfg, mode = "denoise", runs_dir=args["runs_dir"])
            else:
            # if args.get("detect_loss", None) is not None:
            #     cfg[ConfigValue.DETECTLOSS] = args["detect_loss"]

                trainer = DenoiserTrainer(cfg, mode="joint", alpha = args["alpha"], tau = args["tau"], runs_dir=args["runs_dir"])
            # trainer = DenoiserTrainer(cfg, mode="denoise", runs_dir=args["runs_dir"])
        elif args["train_cmd"] == "resume":
            trainer = resume_run(args["run_dir"])
        # elif args["train_cmd"] == "detect":
        #     trainer = detect_run(args["run_dir"])
        # elif args["train_cmd"] == "joint":
        #     trainer = joint_run(args["run_dir"])
        # elif args["train_cmd"] == "ssdet":
        #     trainer = ssdet_run(args["run_dir"])
        else:
            raise NotImplementedError("Invalid train command")

        # Handle shared args
        if args.get("train_dataset", None) is not None:
            trainer.set_train_data(args["train_dataset"])
        if args.get("train_gt", None) is not None:
            trainer.set_train_gt_data(args["train_gt"])
        if args.get("train_label", None) is not None:
            trainer.set_train_label(args["train_label"])
        
        if args.get("validation_dataset", None) is not None:
            trainer.set_test_data(args["validation_dataset"])
        if args.get("validation_gt", None) is not None:
            trainer.set_test_gt_data(args["validation_gt"])
        if args.get("validation_label", None) is not None:
            trainer.set_test_label(args["validation_label"])

        if args.get("iterations", None) is not None:
            cfg[ConfigValue.ITERATIONS] = args["iterations"]
        if args.get("num", None) is not None:
            cfg[ConfigValue.NUM_EVAL] = args["num"]


        if args.get("eval_interval", None) is not None:
            cfg[ConfigValue.EVAL_INTERVAL] = args["eval_interval"]
        if args.get("checkpoint_interval", None) is not None:
            cfg[ConfigValue.SNAPSHOT_INTERVAL] = args["checkpoint_interval"]
        if args.get("print_interval", None) is not None:
            cfg[ConfigValue.PRINT_INTERVAL] = args["print_interval"]
        if args.get("train_batch_size", None) is not None:
            cfg[ConfigValue.TRAIN_MINIBATCH_SIZE] = args["train_batch_size"]
        if args.get("validation_batch_size", None) is not None:
            cfg[ConfigValue.TEST_MINIBATCH_SIZE] = args["validation_batch_size"]
        if args.get("patch_size", None) is not None:
            cfg[ConfigValue.TRAIN_PATCH_SIZE] = args["patch_size"]
        if args.get("alpha", None) is not None:
            cfg[ConfigValue.ALPHA] = args["alpha"]

        if args.get("tau", None) is not None:
            cfg[ConfigValue.TAU] = args["tau"]



        # Start the training
        trainer.train()

    @overrides
    def cmd(self) -> str:
        return "train"
